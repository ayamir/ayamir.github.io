<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>WebXR on Ayamir&#39;s blog</title>
    <link>https://ayamir.github.io/tags/webxr/</link>
    <description>Recent content in WebXR on Ayamir&#39;s blog</description>
    <generator>Hugo 0.125.1</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 25 Apr 2024 21:58:17 +0800</lastBuildDate>
    <atom:link href="https://ayamir.github.io/tags/webxr/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>使用 WebXR 完成基于分块的全景视频自适应码率播放器</title>
      <link>https://ayamir.github.io/posts/development/webxr-for-panoramic-video/</link>
      <pubDate>Fri, 25 Feb 2022 11:04:23 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/webxr-for-panoramic-video/</guid>
      <description>最近几天一直在用WebXR的技术重构目前的基于分块的全景视频自适应码率播放客户端，下面简述一下过程。&#xA;首先结论是：分块播放+自适应码率+完全的沉浸式场景体验=Impossible（直接使用 WebXR 提供的 API）&#xA;分块播放 分块播放的本质是将一整块的全景视频从空间上划分成多个小块，各个小块在时间上与原视频的长度是相同的。&#xA;在实际播放的时候需要将各个小块按照原有的空间顺序排列好之后播放，为了避免各个分块播放进度不同的问题，播放时还需要经过统一的时间同步。&#xA;对应到 web 端的技术实现就是：&#xA;一个分块的视频&amp;lt;-&amp;gt;一个&amp;lt;video&amp;gt;h5 元素&amp;lt;-&amp;gt;一个&amp;lt;canvas&amp;gt;h5 元素&#xA;视频的播放过程就是各个分块对应的&amp;lt;canvas&amp;gt;元素不断重新渲染的过程&#xA;各个分块时间同步的实现需要一个基准视频进行对齐，大体上的原理如下：&#xA;let baseVideo = null; let videos = []; initBaseVideo(); initVideos(); for (video in videos) { video.currentTime = baseVideo.currentTime; } 自适应码率 自适应码率的方案使用dashjs库实现，即对每个分块&amp;lt;video&amp;gt;元素的播放都用dashjs的方案控制：&#xA;import { MediaPlayer } from &amp;#34;dashjs&amp;#34;; let videos = []; let dashs = []; let mpdUrls = []; initVideos(); initMpdUrls(); for (let i = 0; i &amp;lt; tileNum; i++) { let video = videos[i]; let dash = MediaPlayer().</description>
    </item>
  </channel>
</rss>
