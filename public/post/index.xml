<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Ayamir&#39;s blog</title>
    <link>https://ayamir.github.io/post/</link>
    <description>Recent content in Posts on Ayamir&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 26 Apr 2024 09:02:15 +0800</lastBuildDate>
    <atom:link href="https://ayamir.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Jitter Buffer学习理解（上）</title>
      <link>https://ayamir.github.io/posts/knowledge/webrtc/jitter-buffer/</link>
      <pubDate>Thu, 18 Apr 2024 17:33:24 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/webrtc/jitter-buffer/</guid>
      <description>这篇博客主要分析理解 WebRTC 中的 Jitter Buffer 的工作职责以及 Buffer 相关的代码实现。</description>
    </item>
    <item>
      <title>同步、异步、阻塞、非阻塞</title>
      <link>https://ayamir.github.io/posts/knowledge/os/sync-async-block-nonblock/</link>
      <pubDate>Sat, 13 Apr 2024 23:39:22 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/os/sync-async-block-nonblock/</guid>
      <description>这篇博客主要讨论了同步、异步、阻塞和非阻塞这几个编程中常见的概念。</description>
    </item>
    <item>
      <title>进程、线程和协程</title>
      <link>https://ayamir.github.io/posts/knowledge/os/process-thread-coroutine/</link>
      <pubDate>Sat, 06 Apr 2024 19:23:04 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/os/process-thread-coroutine/</guid>
      <description>这篇博客讨论了进程、线程以及协程的概念以及用法。</description>
    </item>
    <item>
      <title>什么是 RPC ？</title>
      <link>https://ayamir.github.io/posts/knowledge/backend/what-is-rpc/</link>
      <pubDate>Fri, 29 Mar 2024 23:55:25 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/backend/what-is-rpc/</guid>
      <description>这篇博客讨论了 RPC 这一概念以及用法。</description>
    </item>
    <item>
      <title>WebRTC任务队列学习笔记</title>
      <link>https://ayamir.github.io/posts/development/webrtc-task-queue/</link>
      <pubDate>Tue, 19 Mar 2024 19:32:57 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/webrtc-task-queue/</guid>
      <description>这篇博客主要学习了 WebRTC 中的任务队列的实现方式和使用方式。</description>
    </item>
    <item>
      <title>虚拟地址空间</title>
      <link>https://ayamir.github.io/posts/knowledge/os/virtual-memory-space/</link>
      <pubDate>Wed, 07 Feb 2024 15:56:52 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/os/virtual-memory-space/</guid>
      <description>这篇博客主要学习了虚拟地址空间的概念以及相关的用法。</description>
    </item>
    <item>
      <title>ABI是什么？</title>
      <link>https://ayamir.github.io/posts/knowledge/cpp/abi/</link>
      <pubDate>Wed, 07 Feb 2024 12:51:01 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/cpp/abi/</guid>
      <description>这篇博客主要学习了 ABI 这一概念以及其相关的用法。</description>
    </item>
    <item>
      <title>孤儿进程</title>
      <link>https://ayamir.github.io/posts/development/orphan-process/</link>
      <pubDate>Mon, 29 Jan 2024 10:31:56 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/orphan-process/</guid>
      <description>问题背景 前两天室友问我，怎么 kill 掉在 Shell 脚本中调用的 Python 进程，我第一时间想到的是：打开 htop，把它调整成树形布局，然后搜索 Shell 脚本，选中之后把它 kill 掉，Python 进程应该也会被 kill 掉。&#xA;但是结果是 Python 进程并没有变红，而是成为了 init 进程的子进程。&#xA;孤儿进程是怎么产生的 大二学 OS 学到父进程和子进程的概念的时候，还是只是以为父进程和子进程之间应该存在牢固的控制关系，父进程退出时子进程也应该默认退出。&#xA;但是 OS 的实际行为不是这样，子进程和父进程只是说明了二者之间存在谁创建谁的关系，并不存在牢固的控制关系（而是类似于现实中的父子关系）。&#xA;父进程结束时子进程并没有结束，子进程成为孤儿进程，会被 init 进程收养&#xA;父进程崩溃或异常终止&#xA;并发和竞争条件导致父子进程的结束顺序错误&#xA;如何避免孤儿进程的产生 其实就是需要在程序设计时，考虑到上述的这几种可能导致孤儿进程产生的原因，然后对异常情况进行注册和处理。对于开始时的这个引入问题而言，答案可以写成以下两个脚本：&#xA;#!/bin/bash # 定义一个函数来处理信号 cleanup() { echo &amp;#34;捕捉到终止信号，正在终止 Python 进程...&amp;#34; kill $PYTHON_PID exit } # 在接收到 SIGINT || SIGTERM || SIGKILL 时执行 cleanup 函数 trap &amp;#39;cleanup&amp;#39; SIGINT SIGTERM # 启动 Python 脚本并获取其进程 ID python example_python.py &amp;amp; PYTHON_PID=$!</description>
    </item>
    <item>
      <title>Git 常用用法记录</title>
      <link>https://ayamir.github.io/posts/development/git-usage/</link>
      <pubDate>Tue, 23 Jan 2024 09:50:29 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/git-usage/</guid>
      <description>这篇博客用来记录平时用到的一些 Git 操作，用到之后会不定时更新。&#xA;clone 相关 克隆指定 branch ： git clone --branch &amp;lt;branch-name&amp;gt; &amp;lt;remote-repo-url&amp;gt;&#xA;递归克隆（包括 submodule ）：git clone --recursive&#xA;已经 clone 完的仓库：git submodule update --init --recursive&#xA;checkout 相关 切换分支：git checkout &amp;lt;branch-name&amp;gt; / git switch &amp;lt;branch-name&amp;gt;&#xA;新建分支：git checkout -b &amp;lt;branch-name&amp;gt; / git switch -c &amp;lt;branch-name&amp;gt;&#xA;切换到一个 tag ：git fetch --all --tags --prune -&amp;gt; git tag -&amp;gt; 使用 / 快速搜索 -&amp;gt; git checkout tags/&amp;lt;tag-name&amp;gt; -b &amp;lt;branch-name&amp;gt;&#xA;commit 相关 undo 本地改动（还未 commit）：git restore &amp;lt;file-path&amp;gt;</description>
    </item>
    <item>
      <title>H264 Encode</title>
      <link>https://ayamir.github.io/posts/knowledge/h264-encode/</link>
      <pubDate>Tue, 23 Jan 2024 01:05:20 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/h264-encode/</guid>
      <description>这篇博客主要总结了与 h264 编码相关的知识。</description>
    </item>
    <item>
      <title>远程桌面与WebRTC</title>
      <link>https://ayamir.github.io/posts/knowledge/webrtc/remote-desktop-with-webrtc/</link>
      <pubDate>Thu, 15 Jun 2023 18:21:02 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/webrtc/remote-desktop-with-webrtc/</guid>
      <description>这篇博客主要分析了远程桌面的原理以及不同的实现方式。</description>
    </item>
    <item>
      <title>在Linux下如何搭建WebRTC的开发环境</title>
      <link>https://ayamir.github.io/posts/development/webrtc-development-prepare/</link>
      <pubDate>Sun, 23 Apr 2023 21:28:38 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/webrtc-development-prepare/</guid>
      <description>本文主要记录笔者在 Gentoo Linux 下面搭建 WebRTC 开发环境的过程。&#xA;准备工作 网络：可以科学上网的梯子 IDE：VSCode 或者 CLion 安装depot_tools Google 有自己的一套用于管理 Chromium 项目的工具，名叫depot_tools，其中有包括git在内的一系列工具和脚本。&#xA;# 创建google目录用于存储google相关的代码 mkdir ~/google cd ~/google # clone depot_tools git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git 克隆完成之后需要将depot_tools的路径加到PATH中，Linux 上添加环境变量最简单的方式是修改~/.profile，这种方式与你的登录 shell 是什么没有关系，不管是fish还是bash还是zsh都会吃这种方式：&#xA;# ~/.profile export GOOGLE_BIN=$HOME/google/depot_tools export PATH=$GOOGLE_BIN:$PATH 但是这种方式需要你注销重新登录。&#xA;克隆代码 mkdir webrtc-checkout cd webrtc-checkout fetch --nohooks webrtc gclient sync 整个 WebRTC 的项目代码大小约 20G，克隆过程中需要保证网络畅通顺畅，如果你的梯子有大流量专用节点最好，否则可能克隆完你的流量就用光了。&#xA;克隆期间可能会因为网络问题中断，重新执行gclient sync即可，直到所有的模块都克隆完毕。&#xA;按照官方的建议，克隆完成之后创建自己的本地分支，因为官方分支更新很快，不 checkout 的话，可能你的 commit 还没写完，就被 Remote 的 change 给覆盖了，还要手动处理冲突。&#xA;cd src git checkout master git new-branch &amp;lt;branch-name&amp;gt; 编译 WebRTC 关于 WebRTC 的版本可以在Chromium Dash查到：</description>
    </item>
    <item>
      <title>WebRTC 中关于视频自适应的相关设置</title>
      <link>https://ayamir.github.io/posts/knowledge/webrtc/note-for-webrtc-1/</link>
      <pubDate>Thu, 15 Sep 2022 20:48:51 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/webrtc/note-for-webrtc-1/</guid>
      <description>这篇博客主要分析理解 WebRTC 中的关于视频自适应机制的相关设置。</description>
    </item>
    <item>
      <title>Note for DQB</title>
      <link>https://ayamir.github.io/posts/papers/note-for-dqb/</link>
      <pubDate>Sun, 20 Mar 2022 22:09:11 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-dqb/</guid>
      <description>&lt;h1 id=&#34;整体概况&#34;&gt;整体概况&lt;/h1&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://ieeexplore.ieee.org/document/9317771&#34;&gt;Modeling the Perceptual Quality for Viewport-Adaptive Omnidirectional Video Streaming Considering Dynamic Quality Boundary Artifact&lt;/a&gt;&#xA;Level：IEEE TCSVT 2021&lt;/p&gt;&#xA;&lt;p&gt;DQB: Dynamic Quality Boundary，指在基于分块的 FoV 自适应全景视频推流过程中低质量分块区域的暴露和质量切换现象。&lt;/p&gt;&#xA;&lt;p&gt;DQB 现象实际上就是 FoV 内分块间的质量差异和随时间变化的分块质量变化。&#xA;这篇论文主要的贡献在于深入研究了这种现象，并且针对此提出了可以利用现存的 QoE 评估指标的模型，并且可以实际应用。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for Toward Immersive Experience</title>
      <link>https://ayamir.github.io/posts/papers/note-for-toward-immersive-experience/</link>
      <pubDate>Wed, 09 Mar 2022 11:20:37 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-toward-immersive-experience/</guid>
      <description>&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;&#xA;&lt;p&gt;Link: &lt;a href=&#34;https://ieeexplore.ieee.org/document/9679801&#34;&gt;Toward Immersive Experience: Evaluation for Interactive Network Services&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level: IEEE Network 2022&lt;/p&gt;&#xA;&lt;p&gt;Keywords: QoE Metrics&lt;/p&gt;</description>
    </item>
    <item>
      <title>MLflow 的用法</title>
      <link>https://ayamir.github.io/posts/development/note-for-mlflow/</link>
      <pubDate>Mon, 07 Mar 2022 19:25:46 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/note-for-mlflow/</guid>
      <description>Overview MLflow是一个用于管理机器学习全生命周期的框架。&#xA;其主要的作用是：&#xA;完成训练和测试过程中不同超参数的结果的记录、对比和可视化——MLflow Tracking 以一种可复现重用的方式包装 ML 代码——MLflow Projects 简化模型部署的难度——MLflow Models 提供中心化的模型存储来管理全生命周期——MLflow Model Registry 现在主要用到的是第三个，所以先记录Models的用法&#xA;MLflow Models MLflow Models本质上是一种格式，用来将机器学习模型包装好之后为下游的工具所用。&#xA;这种格式定义了一种惯例来让我们以不同的flavor保存模型进而可以被下游工具所理解。&#xA;存储格式 每个MLflow Model是一个包含任意文件的目录，根目录之下有一个MLmodel文件，用于定义多个flavor。&#xA;flavor是MLflow Model的关键概念，抽象上是部署工具可以用来理解模型的一种约定。&#xA;MLflow定义了其所有内置部署工具都支持的几种标准flavor，比如描述如何将模型作为Python函数运行的python_function flavor。&#xA;目录结构示例如下：&#xA;MLmode文件内容示例如下：&#xA;这个模型可以用于任何支持pytorch或python_function flavor的工具，例如可以使用如下的命令用python_function来 serve 一个有python_function flavor的模型：&#xA;mlflow models serve -m my_model Model Signature 模型的输入输出要么是column-based，要么是tensor-based。&#xA;column-based inputs and outputs can be described as a sequence of (optionally) named columns with type specified as one of the MLflow data type. tensor-based inputs and outputs can be described as a sequence of (optionally) named tensors with type specified as one of the numpy data type.</description>
    </item>
    <item>
      <title>WebGL 样例的解释</title>
      <link>https://ayamir.github.io/posts/knowledge/webgl/webgl-samples-explanation/</link>
      <pubDate>Thu, 03 Mar 2022 10:31:38 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/webgl/webgl-samples-explanation/</guid>
      <description>这篇博客主要学习总结了 WebGL 的一些使用样例。</description>
    </item>
    <item>
      <title>WebGL 样例</title>
      <link>https://ayamir.github.io/posts/knowledge/webgl/webgl-samples/</link>
      <pubDate>Thu, 03 Mar 2022 10:31:31 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/webgl/webgl-samples/</guid>
      <description>这篇博客主要学习总结了 WebGL 的一些使用样例。</description>
    </item>
    <item>
      <title>WebGL 中的管线</title>
      <link>https://ayamir.github.io/posts/knowledge/webgl/webgl-pipeline/</link>
      <pubDate>Thu, 03 Mar 2022 10:31:22 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/webgl/webgl-pipeline/</guid>
      <description>这篇博客主要学习总结了 WebGL 的管线相关的知识。</description>
    </item>
    <item>
      <title>WebGL 基础知识</title>
      <link>https://ayamir.github.io/posts/knowledge/webgl/webgl-basics/</link>
      <pubDate>Thu, 03 Mar 2022 10:31:04 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/webgl/webgl-basics/</guid>
      <description>这篇博客主要学习总结了 WebGL 的一些基础知识。</description>
    </item>
    <item>
      <title>Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (2)</title>
      <link>https://ayamir.github.io/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/</link>
      <pubDate>Sun, 27 Feb 2022 10:39:45 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-2/</guid>
      <description>&lt;h1 id=&#34;bitrate-adaptation-schemes&#34;&gt;Bitrate Adaptation Schemes&lt;/h1&gt;&#xA;&lt;h2 id=&#34;client-based&#34;&gt;Client-based&lt;/h2&gt;&#xA;&lt;p&gt;Recently, most of the proposed bitrate adaptation schemes reside at the client side, according to the specifications in the DASH standard.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for Survey on Bitrate Adaptation Schemes for Streaming Media Over HTTP (1)</title>
      <link>https://ayamir.github.io/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/</link>
      <pubDate>Sat, 26 Feb 2022 11:26:06 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-survey-on-bitrate-adaptation-schemes-for-streaming-media-over-http-1/</guid>
      <description>&lt;h1 id=&#34;paper-overview&#34;&gt;Paper Overview&lt;/h1&gt;&#xA;&lt;p&gt;Link: &lt;a href=&#34;https://ieeexplore.ieee.org/document/8424813&#34;&gt;https://ieeexplore.ieee.org/document/8424813&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level: IEEE Communications Surveys &amp;amp; Tutorials 2019&lt;/p&gt;</description>
    </item>
    <item>
      <title>使用 WebXR 完成基于分块的全景视频自适应码率播放器</title>
      <link>https://ayamir.github.io/posts/development/webxr-for-panoramic-video/</link>
      <pubDate>Fri, 25 Feb 2022 11:04:23 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/webxr-for-panoramic-video/</guid>
      <description>最近几天一直在用WebXR的技术重构目前的基于分块的全景视频自适应码率播放客户端，下面简述一下过程。&#xA;首先结论是：分块播放+自适应码率+完全的沉浸式场景体验=Impossible（直接使用 WebXR 提供的 API）&#xA;分块播放 分块播放的本质是将一整块的全景视频从空间上划分成多个小块，各个小块在时间上与原视频的长度是相同的。&#xA;在实际播放的时候需要将各个小块按照原有的空间顺序排列好之后播放，为了避免各个分块播放进度不同的问题，播放时还需要经过统一的时间同步。&#xA;对应到 web 端的技术实现就是：&#xA;一个分块的视频&amp;lt;-&amp;gt;一个&amp;lt;video&amp;gt;h5 元素&amp;lt;-&amp;gt;一个&amp;lt;canvas&amp;gt;h5 元素&#xA;视频的播放过程就是各个分块对应的&amp;lt;canvas&amp;gt;元素不断重新渲染的过程&#xA;各个分块时间同步的实现需要一个基准视频进行对齐，大体上的原理如下：&#xA;let baseVideo = null; let videos = []; initBaseVideo(); initVideos(); for (video in videos) { video.currentTime = baseVideo.currentTime; } 自适应码率 自适应码率的方案使用dashjs库实现，即对每个分块&amp;lt;video&amp;gt;元素的播放都用dashjs的方案控制：&#xA;import { MediaPlayer } from &amp;#34;dashjs&amp;#34;; let videos = []; let dashs = []; let mpdUrls = []; initVideos(); initMpdUrls(); for (let i = 0; i &amp;lt; tileNum; i++) { let video = videos[i]; let dash = MediaPlayer().</description>
    </item>
    <item>
      <title>在 Jupyter Notebook 中设置 Conda 环境</title>
      <link>https://ayamir.github.io/posts/development/use-jupyter-notebook-in-conda-env/</link>
      <pubDate>Tue, 15 Feb 2022 17:19:26 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/use-jupyter-notebook-in-conda-env/</guid>
      <description>远程启动jupyter notebool：&#xA;jupyter notebook --no-browser --ip=&amp;#34;&amp;lt;server-ip&amp;gt;&amp;#34; --port=&amp;#34;&amp;lt;server-port&amp;gt;&amp;#34; 激活预先配置好的conda环境，这里假设环境名为keras-tf-2.1.0：&#xA;conda activate keras-tf-2.1.0 安装ipykernel：&#xA;pip3 install ipykernel --user 为ipykernel安装环境：&#xA;python3 -m ipykernel install --user --name=keras-tf-2.1.0 打开notebook更改服务之后刷新即可：</description>
    </item>
    <item>
      <title>Note for Content Based Vp for Live Streaming (2)</title>
      <link>https://ayamir.github.io/posts/papers/note-for-content-based-vp-for-live-streaming-2/</link>
      <pubDate>Tue, 25 Jan 2022 11:59:24 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-content-based-vp-for-live-streaming-2/</guid>
      <description>&lt;h1 id=&#34;liveobj&#34;&gt;LiveObj&lt;/h1&gt;&#xA;&lt;p&gt;&lt;code&gt;LiveDeep&lt;/code&gt;方法利用卷积层从视频内容中提取深层特征，不受动态背景的影响。然而在整个推流会话中需要更新一个带有大量权重的巨大的神经网络模型。同时因为没有历史视频和用户的轨迹的数据，模型需要在运行时从随机权重开始训练。而这会导致两个问题：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;模型需要花很长时间从一次预测错误中恢复；&lt;/li&gt;&#xA;&lt;li&gt;在初始化的阶段预测率成功率很低；&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;为了解决这两个问题，提出预训练的模型来分析视频内容，对视频的语义进行层次化。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;基于对内容的分析，进一步设计了一个轻量级用户模型，将用户偏好映射到不同的视频内容。&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Content Based VP for Live Streaming (1)</title>
      <link>https://ayamir.github.io/posts/papers/note-for-content-based-vp-for-live-streaming-1/</link>
      <pubDate>Sat, 22 Jan 2022 18:03:09 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-content-based-vp-for-live-streaming-1/</guid>
      <description>&lt;h1 id=&#34;livemotion&#34;&gt;LiveMotion&lt;/h1&gt;&#xA;&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;&#xA;&lt;p&gt;基于视频中物体的运动模式来做对应的&lt;code&gt;FoV&lt;/code&gt;预测。&lt;/p&gt;&#xA;&lt;p&gt;将用户的&lt;code&gt;FoV&lt;/code&gt;轨迹与视频内容中运动物体的轨迹结合到一起考虑：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://s2.loli.net/2022/01/26/FRBIAliyvuGDQJp.png&#34; alt=&#34;image-20220126222335930&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;细节可以参见：&lt;a href=&#34;https://ayamir.github.io/posts/note-for-content-motion-viewport-prediction/&#34;&gt;note-for-content-motion-viewport-prediction&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for Popularity Aware 360-Degree Video Streaming</title>
      <link>https://ayamir.github.io/posts/papers/note-for-popularity-aware-360-degree-video-streaming/</link>
      <pubDate>Tue, 18 Jan 2022 16:07:02 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-popularity-aware-360-degree-video-streaming/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9488856/&#34;&gt;Popularity-Aware 360-Degree Video Streaming&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level：IEEE INFOCOM 2021&lt;/p&gt;&#xA;&lt;p&gt;Keywords：Dynamic tiling, Cross-user division, Heuristic QoE optimization&lt;/p&gt;</description>
    </item>
    <item>
      <title>VR 和全景视频的区别总结</title>
      <link>https://ayamir.github.io/posts/knowledge/360video/summary-for-vr-and-panoramic-video/</link>
      <pubDate>Mon, 17 Jan 2022 17:02:51 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/360video/summary-for-vr-and-panoramic-video/</guid>
      <description>这篇博客主要总结了 VR 和全景视频之间的区别。</description>
    </item>
    <item>
      <title>Note for srlABR Cross User</title>
      <link>https://ayamir.github.io/posts/papers/note-for-srlABR-cross-user/</link>
      <pubDate>Sat, 15 Jan 2022 18:46:02 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-srlABR-cross-user/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://ieeexplore.ieee.org/document/9234071&#34;&gt;Sequential Reinforced 360-Degree Video Adaptive Streaming With Cross-User Attentive Network&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level：IEEE Transactions on Broadcasting 2021&lt;/p&gt;&#xA;&lt;p&gt;Keywords：Cross-user vp, Sequential RL ABR&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for 360SRL</title>
      <link>https://ayamir.github.io/posts/papers/note-for-360srl/</link>
      <pubDate>Thu, 13 Jan 2022 12:08:36 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-360srl/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://ieeexplore.ieee.org/document/8784927&#34;&gt;360SRL: A Sequential Reinforcement Learning Approach for ABR Tile-Based 360 Video Streaming&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level：ICME 2019&lt;/p&gt;&#xA;&lt;p&gt;Keywords：ABR、RL、Sequential decision&lt;/p&gt;</description>
    </item>
    <item>
      <title>全景视频中视口预测相关方法总结</title>
      <link>https://ayamir.github.io/posts/knowledge/360video/summary-for-vp/</link>
      <pubDate>Fri, 07 Jan 2022 23:08:36 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/360video/summary-for-vp/</guid>
      <description>这篇博客主要总结了全景视频中视口预测相关方法。</description>
    </item>
    <item>
      <title>Note for Content Assisted Prediction</title>
      <link>https://ayamir.github.io/posts/papers/note-for-content-assisted-prediction/</link>
      <pubDate>Thu, 06 Jan 2022 15:17:33 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-content-assisted-prediction/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://www.researchgate.net/publication/333971523_Content_Assisted_Viewport_Prediction_for_Panoramic_Video_Streaming&#34;&gt;Content Assisted Viewport Prediction for Panoramic Video Streaming&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level：IEEE CVPR 2019 CV4ARVR&lt;/p&gt;&#xA;&lt;p&gt;Keywords：Trajectory-based predict，Content-based predict，Multi-modality fusion&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for GPAC</title>
      <link>https://ayamir.github.io/posts/papers/note-for-gpac/</link>
      <pubDate>Thu, 30 Dec 2021 10:23:26 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-gpac/</guid>
      <description>&lt;h2 id=&#34;dash-客户端自适应逻辑&#34;&gt;Dash 客户端自适应逻辑&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;em&gt;tile priority setup&lt;/em&gt;：根据定义的规则对 tile 进行优先级排名。&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;rate allocation&lt;/em&gt;：收集网络吞吐量信息和 tile 码率信息，使用确定的 tile 优先级排名为其分配码率，努力最大化视频质量。&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;rate adaption&lt;/em&gt;：在播放过程中，执行码率自适应算法，基于播放速度、质量切换的次数、缓冲区占用情况等。&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Note for MPC</title>
      <link>https://ayamir.github.io/posts/papers/note-for-mpc/</link>
      <pubDate>Thu, 23 Dec 2021 10:39:32 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-mpc/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://dl.acm.org/doi/10.1145/2785956.2787486&#34;&gt;A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level：ACM SIGCOMM 15&lt;/p&gt;&#xA;&lt;p&gt;Keywords：Model Predictive Control，ABR，DASH&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for TBRA</title>
      <link>https://ayamir.github.io/posts/papers/note-for-tbra/</link>
      <pubDate>Tue, 21 Dec 2021 10:11:23 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-tbra/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3474085.3475590&#34;&gt;TBRA: Tiling and Bitrate Adaptation for Mobile 360-Degree Video Streaming&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level：ACM MM 21&lt;/p&gt;&#xA;&lt;p&gt;Keywords：Adaptive tiling and bitrate，Mobile streaming&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for Content Motion Viewport Prediction</title>
      <link>https://ayamir.github.io/posts/papers/note-for-content-motion-viewport-prediction/</link>
      <pubDate>Mon, 20 Dec 2021 10:47:18 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-content-motion-viewport-prediction/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3328914&#34;&gt;Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level：Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2019&lt;/p&gt;&#xA;&lt;p&gt;Keywords：Viewport prediction, content-based motion tracking, dynamic user interest model&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for RnnQoE</title>
      <link>https://ayamir.github.io/posts/papers/note-for-rnnQoE/</link>
      <pubDate>Thu, 16 Dec 2021 19:53:10 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-rnnQoE/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://ieeexplore.ieee.org/document/9580281&#34;&gt;QoE-driven Mobile 360 Video Streaming: Predictive&#xA;View Generation and Dynamic Tile Selection&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level：ICCC 2021&lt;/p&gt;&#xA;&lt;p&gt;Keywords：QoE maximization，Trajectory-based viewport prediction，Dynamic tile selection，Differential weight on FOV tiles&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for OpTile</title>
      <link>https://ayamir.github.io/posts/papers/note-for-optile/</link>
      <pubDate>Mon, 13 Dec 2021 16:19:02 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-optile/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link：&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3123266.3123339&#34;&gt;OpTile: Toward Optimal Tiling in 360-degree Video Streaming&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level：ACM MM 17&lt;/p&gt;&#xA;&lt;p&gt;Keyword：Dynamic tile division, Optimize encoding efficiency, Optimize tile size&lt;/p&gt;</description>
    </item>
    <item>
      <title>音视频基础知识</title>
      <link>https://ayamir.github.io/posts/knowledge/mm-base/</link>
      <pubDate>Mon, 13 Dec 2021 10:03:17 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/mm-base/</guid>
      <description>这篇博客主要总结学习了音视频相关的基础知识。</description>
    </item>
    <item>
      <title>Note for RainbowDQN and Multitype Tiles</title>
      <link>https://ayamir.github.io/posts/papers/note-for-rainbowDQN&#43;tiles/</link>
      <pubDate>Sat, 11 Dec 2021 16:14:15 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-rainbowDQN&#43;tiles/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Level：IEEE Transaction on multimedia 21&lt;/p&gt;&#xA;&lt;p&gt;Keyword：Rainbow-DQN, Multi-type tiles, Full streaming system&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for 360ProbDASH</title>
      <link>https://ayamir.github.io/posts/papers/note-for-360ProbDASH/</link>
      <pubDate>Thu, 09 Dec 2021 10:20:15 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-360ProbDASH/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link: &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3123266.3123291&#34;&gt;360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP Adaptive Streaming&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level: ACM MM 17&lt;/p&gt;&#xA;&lt;p&gt;Keyword:&lt;/p&gt;&#xA;&lt;p&gt;Pre-fetch tiles, QoE-driven optimization, Probabilistic model, Rate and Viewport adaptation&lt;/p&gt;</description>
    </item>
    <item>
      <title>Note for Dante</title>
      <link>https://ayamir.github.io/posts/papers/note-for-dante/</link>
      <pubDate>Wed, 08 Dec 2021 22:14:15 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note-for-dante/</guid>
      <description>&lt;h2 id=&#34;论文概况&#34;&gt;论文概况&lt;/h2&gt;&#xA;&lt;p&gt;Link: &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3232565.3234686&#34;&gt;https://dl.acm.org/doi/10.1145/3232565.3234686&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Level: SIGCOMM 18&lt;/p&gt;&#xA;&lt;p&gt;Keyword: UDP+FOV-aware+FEC&lt;/p&gt;</description>
    </item>
    <item>
      <title>沉浸式流媒体传输的实际度量</title>
      <link>https://ayamir.github.io/posts/papers/note11/</link>
      <pubDate>Mon, 22 Nov 2021 15:21:59 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note11/</guid>
      <description>&lt;h2 id=&#34;度量指标&#34;&gt;度量指标&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;viewport 预测精度。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;使用预测的 viewport 坐标和实际用户的 viewport 坐标的大圈距离来量化。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;视频质量。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;viewport 内部的 tile 质量（1～5）。&lt;/li&gt;&#xA;&lt;li&gt;tile 在最高质量层之上花费的时间。&lt;/li&gt;&#xA;&lt;li&gt;根据用户视线的分布而提出的加权质量度量。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>沉浸式推流中应用层的优化</title>
      <link>https://ayamir.github.io/posts/papers/note10/</link>
      <pubDate>Mon, 15 Nov 2021 10:13:18 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note10/</guid>
      <description>&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;&#xA;&lt;p&gt;大多数的 HAS 方案使用 HTTP/1.1 协议进行请求-回应的事务来取得需要的资源、缓冲取到的视频段并以线性的顺序播放。传统的 HAS 中，只需要 1 个 GET 请求来取得下一个视频的暂时的部分。只要视频段的持续时间比网络内的时延高，这种方法就可行。&lt;/p&gt;&#xA;&lt;p&gt;在基于 VR 的 HAS 方案中，播放 1 条视频片段就需要取得多种资源：1 次 GET 请求需要同时请求基础的 tile 层和每个空间视频 tile。使用 4x4 的 tile 方案时，客户端需要发起不少于 17 次 GET 请求。使用 1 s 数量级的分段持续时间，即使是 20 ms 的微小网络延迟也会显着阻碍客户端和服务器之间的整体吞吐量，因此会导致较低的视频质量。&lt;/p&gt;</description>
    </item>
    <item>
      <title>沉浸式流媒体面临的挑战和启示</title>
      <link>https://ayamir.github.io/posts/papers/note9/</link>
      <pubDate>Sun, 14 Nov 2021 19:06:10 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note9/</guid>
      <description>&lt;h2 id=&#34;最终的目标&#34;&gt;最终的目标&lt;/h2&gt;&#xA;&lt;p&gt;主要的挑战是用户的临场感，这可以通过避免虚拟的线索来创造出接近真实的世界。&lt;/p&gt;</description>
    </item>
    <item>
      <title>360度视频的音频处理</title>
      <link>https://ayamir.github.io/posts/papers/note8/</link>
      <pubDate>Sun, 14 Nov 2021 16:52:20 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note8/</guid>
      <description>&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;&#xA;&lt;p&gt;空间音频是一种全球状空间环绕的声音方式，采用多个声音通道来模拟现实世界中听到的声音。&lt;/p&gt;&#xA;&lt;p&gt;360 度视频由于空间音频而变得更加可靠，因为声音的通道特性使其能够穿越时间和空间。&lt;/p&gt;&#xA;&lt;p&gt;360 度视频显示系统在制作空间音频音轨方面的重要性无论怎样强调都不为过&lt;/p&gt;</description>
    </item>
    <item>
      <title>自适应策略之viewport依赖型</title>
      <link>https://ayamir.github.io/posts/papers/note7/</link>
      <pubDate>Sun, 14 Nov 2021 13:24:59 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note7/</guid>
      <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;&#xA;&lt;p&gt;在 360 度视频的推流过程中，根据用户头部的运动自适应地动态选择推流的区域，调整其比特率，以达到节省带宽的目的。&lt;/p&gt;</description>
    </item>
    <item>
      <title>沉浸式流媒体现有标准</title>
      <link>https://ayamir.github.io/posts/papers/note6/</link>
      <pubDate>Thu, 11 Nov 2021 20:08:03 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note6/</guid>
      <description>&lt;h2 id=&#34;omafomnidirectional-media-format&#34;&gt;OMAF(Omnidirectional Media Format)&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;OMAF&lt;/code&gt;是第 1 个国际化的沉浸式媒体格式，描述了对 360 度视频进行编码、演示、消费的方法。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;OMAF&lt;/code&gt;与与现有格式兼容，包括编码（例如&lt;code&gt;HEVC&lt;/code&gt;），文件格式（例如&lt;code&gt;ISOBMFF&lt;/code&gt;），交付信号（例如&lt;code&gt;DASH&lt;/code&gt;，&lt;code&gt;MMT&lt;/code&gt;）。&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;OMAF&lt;/code&gt;中还包括编码、投影、分包和 viewport 方向的元数据。&lt;/p&gt;</description>
    </item>
    <item>
      <title>自适应360度视频推流挑战</title>
      <link>https://ayamir.github.io/posts/papers/note5/</link>
      <pubDate>Thu, 04 Nov 2021 11:01:18 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note5/</guid>
      <description>&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;&#xA;&lt;p&gt;用户使用头戴设备比使用传统显示器观看 360 度视频内容时的满意度对于扰乱更加敏感。&lt;/p&gt;&#xA;&lt;p&gt;沉浸式的体验受到不完美的视角预测和高度动态化的网络状况的消极影响。&lt;/p&gt;</description>
    </item>
    <item>
      <title>沉浸式流媒体网络问题的相关解决方案</title>
      <link>https://ayamir.github.io/posts/papers/note4/</link>
      <pubDate>Sat, 30 Oct 2021 19:20:00 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note4/</guid>
      <description>&lt;h1 id=&#34;概况&#34;&gt;概况&lt;/h1&gt;&#xA;&lt;p&gt;现有的沉浸式流媒体应用都对带宽、QoS 和计算需求有着高要求，这主要得益于 5G 网络。&lt;/p&gt;&#xA;&lt;p&gt;传统的中心化云计算和云存储体系结构不适于实时的高码率内容分发。&lt;/p&gt;&#xA;&lt;p&gt;边缘缓存和移动边缘计算成为了推动沉浸式流媒体发展的关键技术。&lt;/p&gt;</description>
    </item>
    <item>
      <title>重学C&#43;&#43;：容器和迭代器</title>
      <link>https://ayamir.github.io/posts/knowledge/cpp/iterator/</link>
      <pubDate>Thu, 28 Oct 2021 17:09:18 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/cpp/iterator/</guid>
      <description>这篇博客主要讨论了 C++ 中的容器和迭代器。</description>
    </item>
    <item>
      <title>重学C&#43;&#43;：标准库类模板Vector</title>
      <link>https://ayamir.github.io/posts/knowledge/cpp/vector/</link>
      <pubDate>Thu, 28 Oct 2021 15:35:17 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/cpp/vector/</guid>
      <description>这篇博客主要讨论了 C++ 中的 vector 类型的用法。</description>
    </item>
    <item>
      <title>重学C&#43;&#43;：标准库类型string</title>
      <link>https://ayamir.github.io/posts/knowledge/cpp/string/</link>
      <pubDate>Thu, 28 Oct 2021 10:31:33 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/cpp/string/</guid>
      <description>这篇博客主要讨论了 C++ 中的 string 类型的用法。</description>
    </item>
    <item>
      <title>重学C&#43;&#43;：类型推导</title>
      <link>https://ayamir.github.io/posts/knowledge/cpp/auto/</link>
      <pubDate>Tue, 26 Oct 2021 21:14:32 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/cpp/auto/</guid>
      <description>这篇博客主要讨论了 C++ 中类型推导相关的关键字和用法。</description>
    </item>
    <item>
      <title>重学C&#43;&#43;：Const二三事</title>
      <link>https://ayamir.github.io/posts/knowledge/cpp/const/</link>
      <pubDate>Tue, 26 Oct 2021 15:53:11 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/cpp/const/</guid>
      <description>这篇博客主要讨论了 C++ 中 Const 相关的用法。</description>
    </item>
    <item>
      <title>重学C&#43;&#43;：引用和指针</title>
      <link>https://ayamir.github.io/posts/knowledge/cpp/reference-and-pointer/</link>
      <pubDate>Tue, 26 Oct 2021 15:49:49 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/cpp/reference-and-pointer/</guid>
      <description>这篇博客主要讨论了 C++ 中的引用和指针之间的联系和区别。</description>
    </item>
    <item>
      <title>自适应360度视频推流方案</title>
      <link>https://ayamir.github.io/posts/papers/note3/</link>
      <pubDate>Mon, 25 Oct 2021 09:34:10 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note3/</guid>
      <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;&#xA;&lt;p&gt;360 度视频的推流手段逐渐从视角独立型方案变成基于 tile 的视角依赖型方案。&lt;/p&gt;&#xA;&lt;p&gt;相比于常规视频，360 度视频被编码成全向的场景。&lt;/p&gt;&#xA;&lt;p&gt;自适应 360 度视频推流利用 DASH 框架来实现比特率的自适应。&lt;/p&gt;</description>
    </item>
    <item>
      <title>自适应视频推流方案</title>
      <link>https://ayamir.github.io/posts/papers/note2/</link>
      <pubDate>Thu, 21 Oct 2021 10:50:54 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note2/</guid>
      <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;&#xA;&lt;p&gt;自适应方案可以在处理不同目标对象时帮助改善推流体验。&lt;/p&gt;&#xA;&lt;p&gt;目标主要包括视频质量、功耗、负载均衡等在移动无线网和有线网接入的情形。&lt;/p&gt;&#xA;&lt;p&gt;适应性的视频比特率需要同时匹配网络条件和质量目标的需求。&lt;/p&gt;</description>
    </item>
    <item>
      <title>360度流媒体面临的挑战、机遇和解决方案</title>
      <link>https://ayamir.github.io/posts/papers/note1/</link>
      <pubDate>Wed, 20 Oct 2021 20:08:38 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/papers/note1/</guid>
      <description>&lt;h2 id=&#34;360-度流媒体视频框架&#34;&gt;360 度流媒体视频框架&lt;/h2&gt;&#xA;&lt;h3 id=&#34;视频采集和拼接&#34;&gt;视频采集和拼接&lt;/h3&gt;&#xA;&lt;p&gt;使用不同的 360 度视频采集相机可以将视频内容存储为 3D 的球形内容&lt;/p&gt;&#xA;&lt;h3 id=&#34;使用不同的投影策略实现降维&#34;&gt;使用不同的投影策略实现降维&lt;/h3&gt;&#xA;&lt;p&gt;策略主要分为 2 种：视角独立型和视角依赖型&lt;/p&gt;</description>
    </item>
    <item>
      <title>重学C&#43;&#43;：类型系统基础</title>
      <link>https://ayamir.github.io/posts/knowledge/cpp/Cpp-Types/</link>
      <pubDate>Mon, 18 Oct 2021 19:32:22 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/cpp/Cpp-Types/</guid>
      <description>这篇博客主要讨论了 C++ 中的类型系统。</description>
    </item>
    <item>
      <title>部署 Immersive Video OMAF-Sample</title>
      <link>https://ayamir.github.io/posts/development/Immersive-Video-Deploy/</link>
      <pubDate>Sat, 09 Oct 2021 15:31:46 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/Immersive-Video-Deploy/</guid>
      <description>原仓库地址：Immersive-Video-Sample&#xA;修改之后的仓库：Immersive-Video-Sample&#xA;Server 端搭建 修改 Dockerfile 手动设置 wget 和 git 的 http_proxy&#xA;旧 package 目录 not found，修改为新 package 目录&#xA;因为找不到 glog 库因此加入软链接操作&#xA;ln -s /usr/local/lib64/libglog.so.0.6.0 /usr/local/lib64/libglog.so.0 重新编译内核 运行脚本时显示 libnuma 错误因此推断与 numa 设置有关&#xA;执行numactl -H显示只有一个 node，报错输出显示需要至少两个 numa 节点&#xA;查询资料之后获知可以使用 fakenuma 技术创造新节点，但是 Ubuntu 默认的内核没有开启对应的内核参数&#xA;手动下载 Linux 内核源代码到/usr/src/目录 wget https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.11.1.tar.gz 解压 tar xpvf linux-5.11.1.tar.gz 复制现有内核配置 cd linux-5.11.1 &amp;amp;&amp;amp; cp -v /boot/config-$(uname -r) .config 安装必要的包 sudo apt install build-essential libncurses-dev bison flex libssl-dev libelf-dev 进入内核配置界面 sudo make menuconfig 按下/键分别查询CONFIG_NUMA和CONFIG_NUMA_EMU位置 手动勾选对应选项之后保存退出 重新编译并等待安装结束 sudo make -j $(nproc) &amp;amp;&amp;amp; sudo make modules_install &amp;amp;&amp;amp; sudo make install 修改grub启动参数加入 fake numa 配置 sudo vim /etc/default/grub 找到对应行并修改为</description>
    </item>
    <item>
      <title>修复 Archlinux 上出现的 GPGME Error</title>
      <link>https://ayamir.github.io/posts/knowledge/linux/how-to-fix-GPGME-error/</link>
      <pubDate>Fri, 11 Jun 2021 08:50:43 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/linux/how-to-fix-GPGME-error/</guid>
      <description>这篇博客主要描述了解决 Archlinux 上 GPGME Error 问题的方法。</description>
    </item>
    <item>
      <title>在 microsoft-edge-dev 上设置 Python selenium</title>
      <link>https://ayamir.github.io/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/</link>
      <pubDate>Fri, 26 Mar 2021 21:43:35 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/development/python-selenium-settings-on-microsoft-edge-dev-on-linux/</guid>
      <description>Get Correct Version microsoft-edge-dev --version The output is Microsoft Edge 91.0.831.1 dev in my case.&#xA;Get Corresponding WebDriver Find the corresponding version at msedgewebdriverstorage and download the zip.&#xA;Extract it to you path like /usr/local/bin or $HOME/.local/bin.&#xA;Write Code Following is a example.&#xA;from msedge.selenium_tools import EdgeOptions, Edge options = EdgeOptions() options.use_chromium = True options.binary_location = r&amp;#34;/usr/bin/microsoft-edge-dev&amp;#34; options.set_capability(&amp;#34;platform&amp;#34;, &amp;#34;LINUX&amp;#34;) webdriver_path = r&amp;#34;/home/ayamir/.local/bin/msedgewebdriver&amp;#34; browser = Edge(options=options, executable_path=webdriver_path) browser.get(&amp;#34;http://localhost:8000&amp;#34;) assert &amp;#34;Django&amp;#34; in browser.</description>
    </item>
    <item>
      <title>Linux 权限相关命令解读</title>
      <link>https://ayamir.github.io/posts/knowledge/linux/linux-authority/</link>
      <pubDate>Mon, 15 Mar 2021 21:43:35 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/linux/linux-authority/</guid>
      <description>这篇博客主要描述了解决 Linux 上与用户权限相关的命令解读。</description>
    </item>
    <item>
      <title>在 Linux 上手动设置 DNS</title>
      <link>https://ayamir.github.io/posts/knowledge/linux/dns-settings-on-archlinux/</link>
      <pubDate>Tue, 26 Jan 2021 21:43:35 +0800</pubDate>
      <guid>https://ayamir.github.io/posts/knowledge/linux/dns-settings-on-archlinux/</guid>
      <description>这篇博客主要描述了在 Archlinux 上手动设置 DNS 的方式。</description>
    </item>
  </channel>
</rss>
